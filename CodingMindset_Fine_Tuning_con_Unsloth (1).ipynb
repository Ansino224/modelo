{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QySohW7r-XL"
      },
      "source": [
        "# Fine-tuning Llama 3.1 8B LoRA con Unsloth\n",
        "\n",
        "> S√≠gueme en RRSS para apoyar este contenido ‚ù§Ô∏è: <br>\n",
        "x -> [@codingmindsetio](https://x.com/codingmindsetio) <br>\n",
        "YT üé• -> [CodingMindset](https://www.youtube.com/@CodingMindsetIO?sub_confirmation=1) <br>\n",
        "IG üì∏ -> [@codingmindset](https://www.instagram.com/codingmindset?igsh=ZGx5aGd4MXBwYmx5&utm_source=qr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flXkWyXPwG4B"
      },
      "source": [
        "## Instalaci√≥n e importaci√≥n de dependencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64KFNuhawLnq"
      },
      "source": [
        "Para instalar Unsloth en tu propio PC, sigue las instrucciones de instalaci√≥n de la p√°gina de Github [aqu√≠](https://github.com/unslothai/unsloth?tab=readme-ov-file#-installation-instructions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AvlKSc8qr8Gq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Instala Unsloth, Xformers (Flash Attention) y todos los dem√°s paquetes.\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# Tenemos que comprobar qu√© versi√≥n de Torch para Xformers (2.3 -> 0.0.27)\n",
        "from torch import __version__; from packaging.version import Version as V\n",
        "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
        "!pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpjPvGHtwtRv"
      },
      "source": [
        "Comprobamos si tenemos disponible una GPU de Nvidia, si no al importar Unsloth obtendremos un RuntimeError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L-hLfGtpixD",
        "outputId": "ac1e30e4-6f1a-48e6-e2f1-054999eb9a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep  8 23:07:16 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P0              32W /  70W |  11157MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74a-huLKyXSn"
      },
      "source": [
        "Importamos todas las dependecias a utilizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "y6QPKpoPTqtm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHCxEQEny5jD"
      },
      "source": [
        "## Descarga del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhp1x7T9zxoF"
      },
      "source": [
        " ### Par√°metros\n",
        "\n",
        " `max_seq_length`: Al preparar el modelo para su uso, es necesario establecer un l√≠mite m√°ximo para la longitud de las secuencias, lo que afecta su capacidad de procesar informaci√≥n contextual. Aunque la versi√≥n 3.1 de Llama puede manejar contextos de hasta 128 mil tokens, en este caso optaremos por una configuraci√≥n m√°s modesta de 2,048 tokens. Esta elecci√≥n se debe a que utilizar la capacidad m√°xima requiere considerablemente m√°s recursos computacionales y de VRAM.\n",
        " <br>\n",
        " <br>\n",
        " `dtype`: se refiere al tipo de datos que se utilizar√° para representar los n√∫meros en el modelo. Es importante porque afecta la precisi√≥n de los c√°lculos, el uso de memoria y la velocidad de procesamiento.\n",
        "\n",
        "*   **None**: Detecci√≥n autom√°tica\n",
        "*   **torch.float32**: Precisi√≥n est√°ndar, compatible con la mayor√≠a de GPUs\n",
        "*   **torch.float16**: Para GPUs Tesla T4, V100\n",
        "*   **torch.bfloat16**: Para GPUs Ampere y m√°s recientes\n",
        "\n",
        "`load_in_4bit`: Activa la cuantizaci√≥n de 4 bits para ahorrar memoria. Es opcional (puede ser False)\n",
        " <br>\n",
        " <br>\n",
        "`model_name`: En este caso escogemos la versi√≥n pre-cuantizada de Llama de 4-bit, debido a que es mucho m√°s ligera (5.4 GB) comprado con la versi√≥n original con precisi√≥n de 16-bit (16 GB)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ynyjo-rJUFfy"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDQH84LNQN81"
      },
      "source": [
        "## Preparaci√≥n LoRA - PEFT (Parameter Efficient Fine Tuning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItFi0b2hRm7R"
      },
      "source": [
        "LoRA tiene tres par√°metros fundamentales:\n",
        "\n",
        "1. `r` (Rango): Este valor determina el tama√±o de las matrices de LoRA. Normalmente, se empieza con un rango de 8, pero puede llegar hasta 256. Un rango mayor permite almacenar m√°s informaci√≥n, pero tambi√©n aumenta el coste computacional y de memoria. En este caso, hemos optado por un valor de 16.\n",
        "\n",
        "2. `lora_alpha` (Œ±): Es un factor de escala para las actualizaciones. Alfa influye directamente en cu√°nto contribuyen los adaptadores (controla la magnitud de la contribuci√≥n de los adaptadores LoRA a la red principal) y suele establecerse como 1 o 2 veces el valor del rango. En este caso, hemos optado por un valor de 16 (1 * r).\n",
        "\n",
        "3. `target_modules`: LoRA se puede aplicar a varios componentes del modelo, como los mecanismos de atenci√≥n (matrices Q, K, V), proyecciones de salida, bloques feed-forward y capas lineales de salida. Aunque inicialmente se centra en los mecanismos de atenci√≥n, expandir LoRA a otros componentes ha demostrado ser beneficioso. Sin embargo, adaptar m√°s m√≥dulos implica un aumento en el n√∫mero de par√°metros entrenables y en las necesidades de memoria. En este caso,  hemos decidido aplicar LoRA a todos los m√≥dulos lineales para maximizar la calidad.\n",
        "\n",
        "Otros par√°metros:\n",
        "\n",
        "`lora_dropout`: T√©cnica de regularizaci√≥n que desactiva aleatoriamente un porcentaje de conexiones en las matrices de adaptaci√≥n durante el entrenamiento. Su objetivo principal es prevenir el sobreajuste. Ralentiza ligeramente el entrenamiento por lo que en este caso optamos por no usarlo.\n",
        "\n",
        "`use_rslora` (Estabilizador de rango): Introduce una modificaci√≥n en el factor de escala de los adaptadores LoRA. En lugar de usar una proporci√≥n de 1/r, emplea 1/‚àör. Este sutil pero importante cambio tiene dos efectos principales:\n",
        "\n",
        "1. Estabiliza el proceso de aprendizaje, siendo especialmente beneficioso cuando se utilizan rangos de adaptador m√°s altos.\n",
        "\n",
        "2. Permite mejorar el rendimiento del fine-tuning a medida que se incrementa el rango del adaptador.\n",
        "\n",
        "En esencia, rsLoRA busca optimizar el equilibrio entre la capacidad de adaptaci√≥n y la estabilidad del entrenamiento, especialmente al trabajar con configuraciones de LoRA m√°s complejas.\n",
        "\n",
        "`use_gradient_checkpointing`: Unsloth se encarga de gestionar el checkpointing del gradiente. Esta t√©cnica consiste en almacenar temporalmente en el disco duro las capas de embedding de entrada y salida, en lugar de mantenerlas constantemente en la memoria de la tarjeta gr√°fica. El objetivo principal de esta estrategia es optimizar el uso de la VRAM, liberando espacio para otros procesos del modelo durante el entrenamiento.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOv2mW1CQufv"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
        "    use_rslora=True,\n",
        "    use_gradient_checkpointing=\"unsloth\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLTs40iFWt9W"
      },
      "source": [
        "## Preparaci√≥n del Dataset y el Tokenizador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl6LbkuZjMhx"
      },
      "source": [
        "Preparamos el tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1f4Pp2DjQsO"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
        "    chat_template=\"chatml\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKVV8D-DgzoR"
      },
      "source": [
        "En este caso vamos a usar el dataset `\"mlabonne/FineTome-100k\"`. Usa el formato ShareGPT (dataset), ideal para conversaciones multi-turno. Este formato se procesa para extraer pares de instrucci√≥n-respuesta. Luego, los datos se reformatean seg√∫n una plantilla de chat, como ChatML, que estructura la conversaci√≥n. ChatML usa tokens especiales para marcar el inicio y fin de cada mensaje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tIH_m8XzWwnj"
      },
      "outputs": [],
      "source": [
        "def apply_template(examples):\n",
        "    messages = examples[\"conversations\"]\n",
        "    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]\n",
        "    return {\"text\": text}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dEC134mvjbl0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "07fe552ae00f4dc28491b5da765c986b",
            "f6a51e166fa84533af7e37820b55ba7c",
            "9bd6ec158f384387b2f2ea7d3b688c8c",
            "2ab78b9aabe74bc59bd58360f24b92ae",
            "fc5591422bf74e25aea35371c6c60d42",
            "b917692d41094a7e96f0fe6e9d3649ff",
            "b7c6bd11ebc943f387e20458a219c654",
            "e4a20c14f932441c8563c5179ec07321",
            "f0973e6f51ed4b5492a4732d211979d3",
            "a1f2be99b6e44f15a7570caa356c261e",
            "fbf49755b3814ad5a6f2243f227151ff"
          ]
        },
        "outputId": "4fa1ae54-99dd-4cd4-fc1c-a7800a06fef1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/6335 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07fe552ae00f4dc28491b5da765c986b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "dataset = load_dataset(\"mahiatlinux/Reflection-Dataset-ShareGPT-v1\", split=\"train\")\n",
        "dataset = dataset.map(apply_template, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_sjMoZtmKUT"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97Avbxz3mZ9G"
      },
      "source": [
        "\n",
        "\n",
        "Los hiperpar√°metros clave en el entrenamiento de modelos incluyen:\n",
        "\n",
        "1. **`packing` (Empaquetado)**: T√©cnica para combinar m√∫ltiples muestras peque√±as en un lote, mejorando la eficiencia del procesamiento.\n",
        "\n",
        "2. **`learning_rate` (Tasa de aprendizaje)**: Controla la intensidad de las actualizaciones de los par√°metros. Debe equilibrarse para evitar un aprendizaje lento o inestable. Usaremos 0.0003 (3e-4)\n",
        "\n",
        "3. **`lr_scheduler_type` (Planificador de tasa de aprendizaje)**: Ajusta la tasa durante el entrenamiento, generalmente comenzando alta y disminuyendo gradualmente. En este caso nos decantamos por la opcion lineal, una de las opciones m√°s comunes.\n",
        "\n",
        "4. **`per_device_train_batch_size` (Tama√±o del lote)**: Determina cu√°ntas muestras se procesan antes de actualizar los pesos. Lotes m√°s grandes pueden mejorar la estabilidad y velocidad, pero requieren m√°s memoria. El tama√±o del lote ser√° 8.\n",
        "\n",
        "5. **`gradient_accumulation_steps` (Acumulador de gradientes)**: Es una t√©cnica que permite simular un tama√±o de lote m√°s grande sin aumentar el uso de memoria. Funciona acumulando gradientes durante varias pasadas hacia adelante y hacia atr√°s antes de realizar una actualizaci√≥n de los pesos del modelo. Para esta ocasi√≥n, con 2 ser√° suficiente.\n",
        "\n",
        "6. **`num_train_epochs` (N√∫mero de √©pocas)**: Cantidad de veces que el modelo recorre todo el conjunto de datos. M√°s √©pocas pueden mejorar el rendimiento, pero tambi√©n pueden causar sobreajuste. Para ejemplificar usar√© 1 step, una vuelta completa a todo el set de entramiento.\n",
        "\n",
        "7. **`optim` (Optimizador)**: Algoritmo para ajustar los par√°metros del modelo. Se recomienda AdamW de 8 bits por su eficiencia en uso de memoria.\n",
        "\n",
        "8. **`weight_decay` (Decaimiento de pesos)**: T√©cnica de regularizaci√≥n que penaliza pesos grandes para prevenir el sobreajuste.\n",
        "\n",
        "9. **`warmup_steps`** (Pasos de calentamiento): Periodo inicial donde la tasa de aprendizaje aumenta gradualmente, ayudando a estabilizar el entrenamiento.\n",
        "\n",
        "\n",
        "Cada uno de estos hiperpar√°metros juega un papel crucial en el rendimiento y la eficiencia del entrenamiento, y su ajuste adecuado es fundamental para obtener los mejores resultados.\n",
        "\n",
        "Otros par√°metros:\n",
        "- `fp16`: Habilita el entrenamiento en precisi√≥n mixta de 16 bits (si bfloat16 no es compatible).\n",
        "\n",
        "- `bf16`: Habilita el entrenamiento en bfloat16 (si es compatible).\n",
        "\n",
        "- `seed`: Semilla para la generaci√≥n de n√∫meros aleatorios (puede ser cualquier n√∫mero)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "w5pLL944mMZl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0d2628779015475a84657512f41bf8ca",
            "cc433980fcd14f129660cd3335731925",
            "90a32fd74f8e428e88a4c3ea1ec722dd",
            "66b26620d8e74ea5a695253a68797c28",
            "d2ce684937a64d62b163a7e768c9c5e2",
            "9ff31683b474401c92d7504c4369ff6e",
            "d9610cee208a4bda9504f18f991778b6",
            "f27da14569b74a49bf1d22b9ed815e63",
            "c441e54edd074cf2b2aabf68dafbd556",
            "88e549076a5e4a7b8d8338f821440f9d",
            "6ae40ac0629744d09f4e320360d5bc67"
          ]
        },
        "outputId": "3572d05c-292a-4907-e8cf-45a061e4218c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d2628779015475a84657512f41bf8ca"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer=SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=True,\n",
        "    args=TrainingArguments(\n",
        "        learning_rate=3e-4,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        per_device_train_batch_size=3,\n",
        "        gradient_accumulation_steps=2,\n",
        "        num_train_epochs=1,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        warmup_steps=10,\n",
        "        output_dir=\"output\",\n",
        "        seed=0,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EtaICiqy_Xw"
      },
      "source": [
        "Ejecutamos el entranamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "DHDBBB7prnaq",
        "outputId": "7da188ee-96ed-400b-8e22-e666c8c179cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 1,712 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 3 | Gradient Accumulation steps = 2\n",
            "\\        /    Total batch size = 6 | Total steps = 285\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  4/285 01:09 < 2:42:46, 0.03 it/s, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.387700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.300800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzUXmkMJruT9"
      },
      "source": [
        "## Inferencia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnODbOzQ2QZ7"
      },
      "source": [
        "`FastLanguageModel.for_inference()` nos proporciona de manera nativa una inferencia el doble de r√°pida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXTXB1FezCJ_"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QB6FsLx1fjl"
      },
      "source": [
        "Finalmente. realizamos la inferencia:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w76V0P7u1QrD"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"Which number is larger: 9.9 or 9.11\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Uz17UDy08ae"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eNsvkY21ECh"
      },
      "outputs": [],
      "source": [
        "text_streamer = TextStreamer(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9ymNBf51K7_"
      },
      "outputs": [],
      "source": [
        "outputs = model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=1000, use_cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7OtwASY2G6C"
      },
      "source": [
        "## Guardado del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7gV1sm23IB1"
      },
      "source": [
        "Ahora es el momento de guardar nuestro modelo entrenado. Es importante recordar que, debido a la naturaleza de LoRA y QLoRA, lo que realmente hemos entrenado no es el modelo completo, sino un conjunto de adaptadores.\n",
        "\n",
        "Unsloth nos ofrece tres m√©todos para guardar nuestro trabajo:\n",
        "\n",
        "1. `lora`: Este m√©todo guarda √∫nicamente los adaptadores, sin el modelo base.\n",
        "\n",
        "2. `merged_16bit`: Combina los adaptadores con el modelo base y los guarda en precisi√≥n de 16 bits.\n",
        "\n",
        "3. `merged_4bit`: Similar al anterior, pero guarda el modelo combinado en precisi√≥n de 4 bits, lo que resulta en un archivo m√°s compacto.\n",
        "\n",
        "La elecci√≥n del m√©todo depender√° de nuestras necesidades espec√≠ficas de almacenamiento y uso futuro del modelo. En nuestro caso escogemos la opci√≥n de 16bit para mayor precisi√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8EARGUW2JEG"
      },
      "outputs": [],
      "source": [
        "# Guarda el modelo en local\n",
        "model.save_pretrained_merged(\"lora_model\", tokenizer, save_method=\"merged_16bit\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDq0oCyB4CLS"
      },
      "source": [
        "Guardamos el modelo en nuestro repositorio de HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbOndj1p4Bgd"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub_merged(\"Machaxante600/llama-promete-3.1-bit4\", tokenizer, save_method=\"merged_16bit\", token=\"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\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OWDmkKA5IGH"
      },
      "source": [
        "Unsloth ofrece una funcionalidad adicional muy √∫til: la conversi√≥n directa de tu modelo al formato GGUF. Este formato de cuantizaci√≥n, dise√±ado originalmente para llama.cpp, es ampliamente compatible con diversos motores de inferencia, como LM Studio, Ollama, etc.\n",
        "\n",
        "Una de las ventajas del formato GGUF es que permite especificar diferentes niveles de precisi√≥n. Aprovecharemos esta caracter√≠stica para generar m√∫ltiples versiones cuantizadas del modelo. Concretamente, crearemos versiones en las siguientes precisiones: `q2_k`, `q3_k_m`, `q4_k_m`, `q5_k_m`, `q6_k` y `q8_0`.\n",
        "\n",
        "Todas estas versiones cuantizadas se subir√°n a Hugging Face. El repositorio contendr√° todos nuestros archivos GGUF generados.\n",
        "\n",
        "Esta approach nos permite ofrecer una gama de opciones que equilibran el tama√±o del modelo y su precisi√≥n, adapt√°ndose as√≠ a diferentes necesidades de implementaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZIm1VzGD9bA"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub_gguf(\n",
        "        \"Machaxante600/llama-promete-3.1-bit4F\", # Cambia hf por tu nombre de usuario / nombre del repo\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q2_k\", \"q3_k_m\", \"q4_k_m\", \"q5_k_m\", \"q6_k\", \"q8_0\"],\n",
        "        token = \"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\", # Obt√©n tu token de hf en https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhrLlFbY6vhl"
      },
      "source": [
        "## Nota importante"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVEfRKuY61OX"
      },
      "source": [
        "Si has llegado hasta aqu√≠, espero que este tutorial detallado te haya sido de gran ayuda. Si ha sido as√≠, te agradecer√≠a mucho que me ayudases apoyando el contenido para poder seguir creando estos tutoriales tan detallados, ya que llevan muchas horas de trabajo.\n",
        "\n",
        "Apoya el contenido dej√°ndo tu like y un comentario üôèüôè‚ù§Ô∏è\n",
        "\n",
        "Y no olvides seguirme en mis redes!\n",
        "\n",
        "x -> [@codingmindsetio](https://x.com/codingmindsetio) <br>\n",
        "YT üé• -> [CodingMindset](https://www.youtube.com/@CodingMindsetIO?sub_confirmation=1) <br>\n",
        "IG üì∏ -> [@codingmindset](https://www.instagram.com/codingmindset?igsh=ZGx5aGd4MXBwYmx5&utm_source=qr)\n",
        "\n",
        "Muchas gracias de coraz√≥n! ‚ù§Ô∏è"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "quant_methods = [\"q2_k\", \"q3_k_m\", \"q4_k_m\", \"q5_k_m\", \"q6_k\", \"q8_0\"]\n",
        "\n",
        "def push_to_hub_gguf_with_quant(model, tokenizer, hub_path, quant):\n",
        "    model.push_to_hub_gguf(hub_path, tokenizer, quant, token=\"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\")\n",
        "\n",
        "push_model = partial(push_to_hub_gguf_with_quant, model, tokenizer, \"Machaxante600/llama-promete-3.1-bit4\")\n",
        "\n",
        "list(map(push_model, quant_methods))"
      ],
      "metadata": {
        "id": "GpCBc-Rue-Yt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "fHCxEQEny5jD"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07fe552ae00f4dc28491b5da765c986b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6a51e166fa84533af7e37820b55ba7c",
              "IPY_MODEL_9bd6ec158f384387b2f2ea7d3b688c8c",
              "IPY_MODEL_2ab78b9aabe74bc59bd58360f24b92ae"
            ],
            "layout": "IPY_MODEL_fc5591422bf74e25aea35371c6c60d42"
          }
        },
        "f6a51e166fa84533af7e37820b55ba7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b917692d41094a7e96f0fe6e9d3649ff",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b7c6bd11ebc943f387e20458a219c654",
            "value": "Map:‚Äá100%"
          }
        },
        "9bd6ec158f384387b2f2ea7d3b688c8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4a20c14f932441c8563c5179ec07321",
            "max": 6335,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0973e6f51ed4b5492a4732d211979d3",
            "value": 6335
          }
        },
        "2ab78b9aabe74bc59bd58360f24b92ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1f2be99b6e44f15a7570caa356c261e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fbf49755b3814ad5a6f2243f227151ff",
            "value": "‚Äá6335/6335‚Äá[00:00&lt;00:00,‚Äá12900.30‚Äáexamples/s]"
          }
        },
        "fc5591422bf74e25aea35371c6c60d42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b917692d41094a7e96f0fe6e9d3649ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7c6bd11ebc943f387e20458a219c654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4a20c14f932441c8563c5179ec07321": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0973e6f51ed4b5492a4732d211979d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1f2be99b6e44f15a7570caa356c261e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbf49755b3814ad5a6f2243f227151ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d2628779015475a84657512f41bf8ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc433980fcd14f129660cd3335731925",
              "IPY_MODEL_90a32fd74f8e428e88a4c3ea1ec722dd",
              "IPY_MODEL_66b26620d8e74ea5a695253a68797c28"
            ],
            "layout": "IPY_MODEL_d2ce684937a64d62b163a7e768c9c5e2"
          }
        },
        "cc433980fcd14f129660cd3335731925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ff31683b474401c92d7504c4369ff6e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d9610cee208a4bda9504f18f991778b6",
            "value": "Generating‚Äátrain‚Äásplit:‚Äá"
          }
        },
        "90a32fd74f8e428e88a4c3ea1ec722dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f27da14569b74a49bf1d22b9ed815e63",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c441e54edd074cf2b2aabf68dafbd556",
            "value": 1
          }
        },
        "66b26620d8e74ea5a695253a68797c28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88e549076a5e4a7b8d8338f821440f9d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6ae40ac0629744d09f4e320360d5bc67",
            "value": "‚Äá1712/0‚Äá[00:10&lt;00:00,‚Äá204.69‚Äáexamples/s]"
          }
        },
        "d2ce684937a64d62b163a7e768c9c5e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ff31683b474401c92d7504c4369ff6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9610cee208a4bda9504f18f991778b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f27da14569b74a49bf1d22b9ed815e63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c441e54edd074cf2b2aabf68dafbd556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88e549076a5e4a7b8d8338f821440f9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ae40ac0629744d09f4e320360d5bc67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}