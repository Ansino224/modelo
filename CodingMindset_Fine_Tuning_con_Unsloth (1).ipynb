{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QySohW7r-XL"
      },
      "source": [
        "# Fine-tuning Llama 3.1 8B LoRA con Unsloth\n",
        "\n",
        "> Sígueme en RRSS para apoyar este contenido ❤️: <br>\n",
        "x -> [@codingmindsetio](https://x.com/codingmindsetio) <br>\n",
        "YT 🎥 -> [CodingMindset](https://www.youtube.com/@CodingMindsetIO?sub_confirmation=1) <br>\n",
        "IG 📸 -> [@codingmindset](https://www.instagram.com/codingmindset?igsh=ZGx5aGd4MXBwYmx5&utm_source=qr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flXkWyXPwG4B"
      },
      "source": [
        "## Instalación e importación de dependencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64KFNuhawLnq"
      },
      "source": [
        "Para instalar Unsloth en tu propio PC, sigue las instrucciones de instalación de la página de Github [aquí](https://github.com/unslothai/unsloth?tab=readme-ov-file#-installation-instructions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AvlKSc8qr8Gq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Instala Unsloth, Xformers (Flash Attention) y todos los demás paquetes.\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "# Tenemos que comprobar qué versión de Torch para Xformers (2.3 -> 0.0.27)\n",
        "from torch import __version__; from packaging.version import Version as V\n",
        "xformers = \"xformers==0.0.27\" if V(__version__) < V(\"2.4.0\") else \"xformers\"\n",
        "!pip install --no-deps {xformers} trl peft accelerate bitsandbytes triton"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpjPvGHtwtRv"
      },
      "source": [
        "Comprobamos si tenemos disponible una GPU de Nvidia, si no al importar Unsloth obtendremos un RuntimeError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L-hLfGtpixD",
        "outputId": "ac1e30e4-6f1a-48e6-e2f1-054999eb9a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Sep  8 23:07:16 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P0              32W /  70W |  11157MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74a-huLKyXSn"
      },
      "source": [
        "Importamos todas las dependecias a utilizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "y6QPKpoPTqtm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments, TextStreamer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHCxEQEny5jD"
      },
      "source": [
        "## Descarga del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vhp1x7T9zxoF"
      },
      "source": [
        " ### Parámetros\n",
        "\n",
        " `max_seq_length`: Al preparar el modelo para su uso, es necesario establecer un límite máximo para la longitud de las secuencias, lo que afecta su capacidad de procesar información contextual. Aunque la versión 3.1 de Llama puede manejar contextos de hasta 128 mil tokens, en este caso optaremos por una configuración más modesta de 2,048 tokens. Esta elección se debe a que utilizar la capacidad máxima requiere considerablemente más recursos computacionales y de VRAM.\n",
        " <br>\n",
        " <br>\n",
        " `dtype`: se refiere al tipo de datos que se utilizará para representar los números en el modelo. Es importante porque afecta la precisión de los cálculos, el uso de memoria y la velocidad de procesamiento.\n",
        "\n",
        "*   **None**: Detección automática\n",
        "*   **torch.float32**: Precisión estándar, compatible con la mayoría de GPUs\n",
        "*   **torch.float16**: Para GPUs Tesla T4, V100\n",
        "*   **torch.bfloat16**: Para GPUs Ampere y más recientes\n",
        "\n",
        "`load_in_4bit`: Activa la cuantización de 4 bits para ahorrar memoria. Es opcional (puede ser False)\n",
        " <br>\n",
        " <br>\n",
        "`model_name`: En este caso escogemos la versión pre-cuantizada de Llama de 4-bit, debido a que es mucho más ligera (5.4 GB) comprado con la versión original con precisión de 16-bit (16 GB)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ynyjo-rJUFfy"
      },
      "outputs": [],
      "source": [
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,\n",
        "    dtype=None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDQH84LNQN81"
      },
      "source": [
        "## Preparación LoRA - PEFT (Parameter Efficient Fine Tuning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItFi0b2hRm7R"
      },
      "source": [
        "LoRA tiene tres parámetros fundamentales:\n",
        "\n",
        "1. `r` (Rango): Este valor determina el tamaño de las matrices de LoRA. Normalmente, se empieza con un rango de 8, pero puede llegar hasta 256. Un rango mayor permite almacenar más información, pero también aumenta el coste computacional y de memoria. En este caso, hemos optado por un valor de 16.\n",
        "\n",
        "2. `lora_alpha` (α): Es un factor de escala para las actualizaciones. Alfa influye directamente en cuánto contribuyen los adaptadores (controla la magnitud de la contribución de los adaptadores LoRA a la red principal) y suele establecerse como 1 o 2 veces el valor del rango. En este caso, hemos optado por un valor de 16 (1 * r).\n",
        "\n",
        "3. `target_modules`: LoRA se puede aplicar a varios componentes del modelo, como los mecanismos de atención (matrices Q, K, V), proyecciones de salida, bloques feed-forward y capas lineales de salida. Aunque inicialmente se centra en los mecanismos de atención, expandir LoRA a otros componentes ha demostrado ser beneficioso. Sin embargo, adaptar más módulos implica un aumento en el número de parámetros entrenables y en las necesidades de memoria. En este caso,  hemos decidido aplicar LoRA a todos los módulos lineales para maximizar la calidad.\n",
        "\n",
        "Otros parámetros:\n",
        "\n",
        "`lora_dropout`: Técnica de regularización que desactiva aleatoriamente un porcentaje de conexiones en las matrices de adaptación durante el entrenamiento. Su objetivo principal es prevenir el sobreajuste. Ralentiza ligeramente el entrenamiento por lo que en este caso optamos por no usarlo.\n",
        "\n",
        "`use_rslora` (Estabilizador de rango): Introduce una modificación en el factor de escala de los adaptadores LoRA. En lugar de usar una proporción de 1/r, emplea 1/√r. Este sutil pero importante cambio tiene dos efectos principales:\n",
        "\n",
        "1. Estabiliza el proceso de aprendizaje, siendo especialmente beneficioso cuando se utilizan rangos de adaptador más altos.\n",
        "\n",
        "2. Permite mejorar el rendimiento del fine-tuning a medida que se incrementa el rango del adaptador.\n",
        "\n",
        "En esencia, rsLoRA busca optimizar el equilibrio entre la capacidad de adaptación y la estabilidad del entrenamiento, especialmente al trabajar con configuraciones de LoRA más complejas.\n",
        "\n",
        "`use_gradient_checkpointing`: Unsloth se encarga de gestionar el checkpointing del gradiente. Esta técnica consiste en almacenar temporalmente en el disco duro las capas de embedding de entrada y salida, en lugar de mantenerlas constantemente en la memoria de la tarjeta gráfica. El objetivo principal de esta estrategia es optimizar el uso de la VRAM, liberando espacio para otros procesos del modelo durante el entrenamiento.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOv2mW1CQufv"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
        "    use_rslora=True,\n",
        "    use_gradient_checkpointing=\"unsloth\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLTs40iFWt9W"
      },
      "source": [
        "## Preparación del Dataset y el Tokenizador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cl6LbkuZjMhx"
      },
      "source": [
        "Preparamos el tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1f4Pp2DjQsO"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
        "    chat_template=\"chatml\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKVV8D-DgzoR"
      },
      "source": [
        "En este caso vamos a usar el dataset `\"mlabonne/FineTome-100k\"`. Usa el formato ShareGPT (dataset), ideal para conversaciones multi-turno. Este formato se procesa para extraer pares de instrucción-respuesta. Luego, los datos se reformatean según una plantilla de chat, como ChatML, que estructura la conversación. ChatML usa tokens especiales para marcar el inicio y fin de cada mensaje."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "tIH_m8XzWwnj"
      },
      "outputs": [],
      "source": [
        "def apply_template(examples):\n",
        "    messages = examples[\"conversations\"]\n",
        "    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]\n",
        "    return {\"text\": text}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dEC134mvjbl0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "07fe552ae00f4dc28491b5da765c986b",
            "f6a51e166fa84533af7e37820b55ba7c",
            "9bd6ec158f384387b2f2ea7d3b688c8c",
            "2ab78b9aabe74bc59bd58360f24b92ae",
            "fc5591422bf74e25aea35371c6c60d42",
            "b917692d41094a7e96f0fe6e9d3649ff",
            "b7c6bd11ebc943f387e20458a219c654",
            "e4a20c14f932441c8563c5179ec07321",
            "f0973e6f51ed4b5492a4732d211979d3",
            "a1f2be99b6e44f15a7570caa356c261e",
            "fbf49755b3814ad5a6f2243f227151ff"
          ]
        },
        "outputId": "4fa1ae54-99dd-4cd4-fc1c-a7800a06fef1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/6335 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "07fe552ae00f4dc28491b5da765c986b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "dataset = load_dataset(\"mahiatlinux/Reflection-Dataset-ShareGPT-v1\", split=\"train\")\n",
        "dataset = dataset.map(apply_template, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_sjMoZtmKUT"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97Avbxz3mZ9G"
      },
      "source": [
        "\n",
        "\n",
        "Los hiperparámetros clave en el entrenamiento de modelos incluyen:\n",
        "\n",
        "1. **`packing` (Empaquetado)**: Técnica para combinar múltiples muestras pequeñas en un lote, mejorando la eficiencia del procesamiento.\n",
        "\n",
        "2. **`learning_rate` (Tasa de aprendizaje)**: Controla la intensidad de las actualizaciones de los parámetros. Debe equilibrarse para evitar un aprendizaje lento o inestable. Usaremos 0.0003 (3e-4)\n",
        "\n",
        "3. **`lr_scheduler_type` (Planificador de tasa de aprendizaje)**: Ajusta la tasa durante el entrenamiento, generalmente comenzando alta y disminuyendo gradualmente. En este caso nos decantamos por la opcion lineal, una de las opciones más comunes.\n",
        "\n",
        "4. **`per_device_train_batch_size` (Tamaño del lote)**: Determina cuántas muestras se procesan antes de actualizar los pesos. Lotes más grandes pueden mejorar la estabilidad y velocidad, pero requieren más memoria. El tamaño del lote será 8.\n",
        "\n",
        "5. **`gradient_accumulation_steps` (Acumulador de gradientes)**: Es una técnica que permite simular un tamaño de lote más grande sin aumentar el uso de memoria. Funciona acumulando gradientes durante varias pasadas hacia adelante y hacia atrás antes de realizar una actualización de los pesos del modelo. Para esta ocasión, con 2 será suficiente.\n",
        "\n",
        "6. **`num_train_epochs` (Número de épocas)**: Cantidad de veces que el modelo recorre todo el conjunto de datos. Más épocas pueden mejorar el rendimiento, pero también pueden causar sobreajuste. Para ejemplificar usaré 1 step, una vuelta completa a todo el set de entramiento.\n",
        "\n",
        "7. **`optim` (Optimizador)**: Algoritmo para ajustar los parámetros del modelo. Se recomienda AdamW de 8 bits por su eficiencia en uso de memoria.\n",
        "\n",
        "8. **`weight_decay` (Decaimiento de pesos)**: Técnica de regularización que penaliza pesos grandes para prevenir el sobreajuste.\n",
        "\n",
        "9. **`warmup_steps`** (Pasos de calentamiento): Periodo inicial donde la tasa de aprendizaje aumenta gradualmente, ayudando a estabilizar el entrenamiento.\n",
        "\n",
        "\n",
        "Cada uno de estos hiperparámetros juega un papel crucial en el rendimiento y la eficiencia del entrenamiento, y su ajuste adecuado es fundamental para obtener los mejores resultados.\n",
        "\n",
        "Otros parámetros:\n",
        "- `fp16`: Habilita el entrenamiento en precisión mixta de 16 bits (si bfloat16 no es compatible).\n",
        "\n",
        "- `bf16`: Habilita el entrenamiento en bfloat16 (si es compatible).\n",
        "\n",
        "- `seed`: Semilla para la generación de números aleatorios (puede ser cualquier número)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "w5pLL944mMZl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0d2628779015475a84657512f41bf8ca",
            "cc433980fcd14f129660cd3335731925",
            "90a32fd74f8e428e88a4c3ea1ec722dd",
            "66b26620d8e74ea5a695253a68797c28",
            "d2ce684937a64d62b163a7e768c9c5e2",
            "9ff31683b474401c92d7504c4369ff6e",
            "d9610cee208a4bda9504f18f991778b6",
            "f27da14569b74a49bf1d22b9ed815e63",
            "c441e54edd074cf2b2aabf68dafbd556",
            "88e549076a5e4a7b8d8338f821440f9d",
            "6ae40ac0629744d09f4e320360d5bc67"
          ]
        },
        "outputId": "3572d05c-292a-4907-e8cf-45a061e4218c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d2628779015475a84657512f41bf8ca"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer=SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dataset_num_proc=2,\n",
        "    packing=True,\n",
        "    args=TrainingArguments(\n",
        "        learning_rate=3e-4,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        per_device_train_batch_size=3,\n",
        "        gradient_accumulation_steps=2,\n",
        "        num_train_epochs=1,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        warmup_steps=10,\n",
        "        output_dir=\"output\",\n",
        "        seed=0,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EtaICiqy_Xw"
      },
      "source": [
        "Ejecutamos el entranamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "DHDBBB7prnaq",
        "outputId": "7da188ee-96ed-400b-8e22-e666c8c179cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 1,712 | Num Epochs = 1\n",
            "O^O/ \\_/ \\    Batch size per device = 3 | Gradient Accumulation steps = 2\n",
            "\\        /    Total batch size = 6 | Total steps = 285\n",
            " \"-____-\"     Number of trainable parameters = 41,943,040\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='285' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  4/285 01:09 < 2:42:46, 0.03 it/s, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.387700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.300800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzUXmkMJruT9"
      },
      "source": [
        "## Inferencia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnODbOzQ2QZ7"
      },
      "source": [
        "`FastLanguageModel.for_inference()` nos proporciona de manera nativa una inferencia el doble de rápida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXTXB1FezCJ_"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.for_inference(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QB6FsLx1fjl"
      },
      "source": [
        "Finalmente. realizamos la inferencia:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w76V0P7u1QrD"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"Which number is larger: 9.9 or 9.11\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Uz17UDy08ae"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eNsvkY21ECh"
      },
      "outputs": [],
      "source": [
        "text_streamer = TextStreamer(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9ymNBf51K7_"
      },
      "outputs": [],
      "source": [
        "outputs = model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=1000, use_cache=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7OtwASY2G6C"
      },
      "source": [
        "## Guardado del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7gV1sm23IB1"
      },
      "source": [
        "Ahora es el momento de guardar nuestro modelo entrenado. Es importante recordar que, debido a la naturaleza de LoRA y QLoRA, lo que realmente hemos entrenado no es el modelo completo, sino un conjunto de adaptadores.\n",
        "\n",
        "Unsloth nos ofrece tres métodos para guardar nuestro trabajo:\n",
        "\n",
        "1. `lora`: Este método guarda únicamente los adaptadores, sin el modelo base.\n",
        "\n",
        "2. `merged_16bit`: Combina los adaptadores con el modelo base y los guarda en precisión de 16 bits.\n",
        "\n",
        "3. `merged_4bit`: Similar al anterior, pero guarda el modelo combinado en precisión de 4 bits, lo que resulta en un archivo más compacto.\n",
        "\n",
        "La elección del método dependerá de nuestras necesidades específicas de almacenamiento y uso futuro del modelo. En nuestro caso escogemos la opción de 16bit para mayor precisión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8EARGUW2JEG"
      },
      "outputs": [],
      "source": [
        "# Guarda el modelo en local\n",
        "model.save_pretrained_merged(\"lora_model\", tokenizer, save_method=\"merged_16bit\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDq0oCyB4CLS"
      },
      "source": [
        "Guardamos el modelo en nuestro repositorio de HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbOndj1p4Bgd"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub_merged(\"Machaxante600/llama-promete-3.1-bit4\", tokenizer, save_method=\"merged_16bit\", token=\"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\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OWDmkKA5IGH"
      },
      "source": [
        "Unsloth ofrece una funcionalidad adicional muy útil: la conversión directa de tu modelo al formato GGUF. Este formato de cuantización, diseñado originalmente para llama.cpp, es ampliamente compatible con diversos motores de inferencia, como LM Studio, Ollama, etc.\n",
        "\n",
        "Una de las ventajas del formato GGUF es que permite especificar diferentes niveles de precisión. Aprovecharemos esta característica para generar múltiples versiones cuantizadas del modelo. Concretamente, crearemos versiones en las siguientes precisiones: `q2_k`, `q3_k_m`, `q4_k_m`, `q5_k_m`, `q6_k` y `q8_0`.\n",
        "\n",
        "Todas estas versiones cuantizadas se subirán a Hugging Face. El repositorio contendrá todos nuestros archivos GGUF generados.\n",
        "\n",
        "Esta approach nos permite ofrecer una gama de opciones que equilibran el tamaño del modelo y su precisión, adaptándose así a diferentes necesidades de implementación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZIm1VzGD9bA"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub_gguf(\n",
        "        \"Machaxante600/llama-promete-3.1-bit4F\", # Cambia hf por tu nombre de usuario / nombre del repo\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q2_k\", \"q3_k_m\", \"q4_k_m\", \"q5_k_m\", \"q6_k\", \"q8_0\"],\n",
        "        token = \"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\", # Obtén tu token de hf en https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhrLlFbY6vhl"
      },
      "source": [
        "## Nota importante"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVEfRKuY61OX"
      },
      "source": [
        "Si has llegado hasta aquí, espero que este tutorial detallado te haya sido de gran ayuda. Si ha sido así, te agradecería mucho que me ayudases apoyando el contenido para poder seguir creando estos tutoriales tan detallados, ya que llevan muchas horas de trabajo.\n",
        "\n",
        "Apoya el contenido dejándo tu like y un comentario 🙏🙏❤️\n",
        "\n",
        "Y no olvides seguirme en mis redes!\n",
        "\n",
        "x -> [@codingmindsetio](https://x.com/codingmindsetio) <br>\n",
        "YT 🎥 -> [CodingMindset](https://www.youtube.com/@CodingMindsetIO?sub_confirmation=1) <br>\n",
        "IG 📸 -> [@codingmindset](https://www.instagram.com/codingmindset?igsh=ZGx5aGd4MXBwYmx5&utm_source=qr)\n",
        "\n",
        "Muchas gracias de corazón! ❤️"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "\n",
        "quant_methods = [\"q2_k\", \"q3_k_m\", \"q4_k_m\", \"q5_k_m\", \"q6_k\", \"q8_0\"]\n",
        "\n",
        "def push_to_hub_gguf_with_quant(model, tokenizer, hub_path, quant):\n",
        "    model.push_to_hub_gguf(hub_path, tokenizer, quant, token=\"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\")\n",
        "\n",
        "push_model = partial(push_to_hub_gguf_with_quant, model, tokenizer, \"Machaxante600/llama-promete-3.1-bit4\")\n",
        "\n",
        "list(map(push_model, quant_methods))"
      ],
      "metadata": {
        "id": "GpCBc-Rue-Yt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "fHCxEQEny5jD"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07fe552ae00f4dc28491b5da765c986b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f6a51e166fa84533af7e37820b55ba7c",
              "IPY_MODEL_9bd6ec158f384387b2f2ea7d3b688c8c",
              "IPY_MODEL_2ab78b9aabe74bc59bd58360f24b92ae"
            ],
            "layout": "IPY_MODEL_fc5591422bf74e25aea35371c6c60d42"
          }
        },
        "f6a51e166fa84533af7e37820b55ba7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b917692d41094a7e96f0fe6e9d3649ff",
            "placeholder": "​",
            "style": "IPY_MODEL_b7c6bd11ebc943f387e20458a219c654",
            "value": "Map: 100%"
          }
        },
        "9bd6ec158f384387b2f2ea7d3b688c8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4a20c14f932441c8563c5179ec07321",
            "max": 6335,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0973e6f51ed4b5492a4732d211979d3",
            "value": 6335
          }
        },
        "2ab78b9aabe74bc59bd58360f24b92ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1f2be99b6e44f15a7570caa356c261e",
            "placeholder": "​",
            "style": "IPY_MODEL_fbf49755b3814ad5a6f2243f227151ff",
            "value": " 6335/6335 [00:00&lt;00:00, 12900.30 examples/s]"
          }
        },
        "fc5591422bf74e25aea35371c6c60d42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b917692d41094a7e96f0fe6e9d3649ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7c6bd11ebc943f387e20458a219c654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4a20c14f932441c8563c5179ec07321": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0973e6f51ed4b5492a4732d211979d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1f2be99b6e44f15a7570caa356c261e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbf49755b3814ad5a6f2243f227151ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d2628779015475a84657512f41bf8ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc433980fcd14f129660cd3335731925",
              "IPY_MODEL_90a32fd74f8e428e88a4c3ea1ec722dd",
              "IPY_MODEL_66b26620d8e74ea5a695253a68797c28"
            ],
            "layout": "IPY_MODEL_d2ce684937a64d62b163a7e768c9c5e2"
          }
        },
        "cc433980fcd14f129660cd3335731925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ff31683b474401c92d7504c4369ff6e",
            "placeholder": "​",
            "style": "IPY_MODEL_d9610cee208a4bda9504f18f991778b6",
            "value": "Generating train split: "
          }
        },
        "90a32fd74f8e428e88a4c3ea1ec722dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f27da14569b74a49bf1d22b9ed815e63",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c441e54edd074cf2b2aabf68dafbd556",
            "value": 1
          }
        },
        "66b26620d8e74ea5a695253a68797c28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88e549076a5e4a7b8d8338f821440f9d",
            "placeholder": "​",
            "style": "IPY_MODEL_6ae40ac0629744d09f4e320360d5bc67",
            "value": " 1712/0 [00:10&lt;00:00, 204.69 examples/s]"
          }
        },
        "d2ce684937a64d62b163a7e768c9c5e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ff31683b474401c92d7504c4369ff6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9610cee208a4bda9504f18f991778b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f27da14569b74a49bf1d22b9ed815e63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c441e54edd074cf2b2aabf68dafbd556": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88e549076a5e4a7b8d8338f821440f9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ae40ac0629744d09f4e320360d5bc67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}